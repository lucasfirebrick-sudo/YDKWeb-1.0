#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆäº§å“é¡µé¢å®¡è®¡å·¥å…·
å…¨æ–¹ä½æ£€æŸ¥39ä¸ªäº§å“é¡µé¢çš„æ‰€æœ‰åŠŸèƒ½å’Œé—®é¢˜
"""

import os
import re
import json
from bs4 import BeautifulSoup
from datetime import datetime

def main():
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    products_dir = os.path.join(base_dir, 'products')
    images_dir = os.path.join(base_dir, 'images', 'products')

    print("ğŸ” å¼€å§‹å…¨æ–¹ä½äº§å“é¡µé¢å®¡è®¡...")
    print(f"ğŸ“… å®¡è®¡æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)

    # è·å–æ‰€æœ‰äº§å“é¡µé¢
    product_files = [f for f in os.listdir(products_dir) if f.endswith('.html')]

    print(f"ğŸ“‹ å‘ç° {len(product_files)} ä¸ªäº§å“é¡µé¢")

    # å­˜å‚¨æ‰€æœ‰å®¡è®¡ç»“æœ
    audit_results = {}

    # é€ä¸ªäº§å“è¿›è¡Œå…¨é¢å®¡è®¡
    for i, filename in enumerate(sorted(product_files), 1):
        product_id = filename.replace('.html', '')
        print(f"\n[{i:2d}/{len(product_files)}] ğŸ” å®¡è®¡äº§å“: {product_id}")

        result = comprehensive_product_audit(product_id, products_dir, images_dir)
        audit_results[product_id] = result

        # æ˜¾ç¤ºç®€è¦ç»“æœ
        print_brief_result(product_id, result)

    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    generate_comprehensive_report(audit_results, base_dir)

    # ç”Ÿæˆé—®é¢˜ä¿®å¤æ¸…å•
    generate_fix_plan(audit_results, base_dir)

def comprehensive_product_audit(product_id, products_dir, images_dir):
    """å¯¹å•ä¸ªäº§å“è¿›è¡Œå…¨é¢å®¡è®¡"""
    result = {
        'product_id': product_id,
        'audit_time': datetime.now().isoformat(),
        'file_exists': False,
        'html_structure': {},
        'image_analysis': {},
        'javascript_check': {},
        'carousel_analysis': {},
        'quote_function': {},
        'css_references': {},
        'issues': [],
        'severity': 'unknown',
        'fix_priority': 0
    }

    filepath = os.path.join(products_dir, f'{product_id}.html')

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(filepath):
        result['issues'].append({'type': 'critical', 'message': 'äº§å“æ–‡ä»¶ä¸å­˜åœ¨'})
        result['severity'] = 'critical'
        return result

    result['file_exists'] = True

    # è¯»å–æ–‡ä»¶å†…å®¹
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    # è§£æHTML
    soup = BeautifulSoup(content, 'html.parser')

    # 1. HTMLç»“æ„æ£€æŸ¥
    result['html_structure'] = check_html_structure(soup, content)

    # 2. å›¾ç‰‡åˆ†æ
    result['image_analysis'] = analyze_images(soup, content, images_dir)

    # 3. JavaScriptæ£€æŸ¥
    result['javascript_check'] = check_javascript_references(content)

    # 4. è½®æ’­åŠŸèƒ½åˆ†æ
    result['carousel_analysis'] = analyze_carousel_config(soup, content, images_dir)

    # 5. è·å–æŠ¥ä»·åŠŸèƒ½æ£€æŸ¥
    result['quote_function'] = check_quote_function(soup, content, product_id)

    # 6. CSSå¼•ç”¨æ£€æŸ¥
    result['css_references'] = check_css_references(content)

    # 7. ç»¼åˆé—®é¢˜åˆ†æ
    result['issues'] = compile_issues(result)

    # 8. ç¡®å®šä¸¥é‡ç¨‹åº¦å’Œä¿®å¤ä¼˜å…ˆçº§
    result['severity'], result['fix_priority'] = assess_severity(result)

    return result

def check_html_structure(soup, content):
    """æ£€æŸ¥HTMLç»“æ„å®Œæ•´æ€§"""
    structure = {
        'has_main_image': False,
        'has_product_title': False,
        'has_product_info': False,
        'has_tabs_section': False,
        'has_cta_buttons': False,
        'unclosed_tags': [],
        'nested_issues': []
    }

    # æ£€æŸ¥ä¸»è¦å…ƒç´ 
    structure['has_main_image'] = bool(soup.find('img', class_='main-image'))
    structure['has_product_title'] = bool(soup.find('h1', class_='product-title'))
    structure['has_product_info'] = bool(soup.find('div', class_='product-info'))
    structure['has_tabs_section'] = bool(soup.find('section', class_='product-details-tabs-section'))
    structure['has_cta_buttons'] = bool(soup.find('div', class_='cta-section'))

    # æ£€æŸ¥æœªé—­åˆæ ‡ç­¾
    img_tags = re.findall(r'<img[^>]*(?<!/)>', content)
    for tag in img_tags:
        if not tag.endswith('/>') and '/>' not in tag:
            structure['unclosed_tags'].append(tag[:50] + '...' if len(tag) > 50 else tag)

    return structure

def analyze_images(soup, content, images_dir):
    """åˆ†æå›¾ç‰‡é…ç½®å’Œæ–‡ä»¶å­˜åœ¨æ€§"""
    analysis = {
        'main_image_src': '',
        'data_images_config': '',
        'total_images_found': 0,
        'existing_images': [],
        'missing_images': [],
        'duplicate_images': [],
        'placeholder_issues': []
    }

    # è·å–ä¸»å›¾ç‰‡
    main_img = soup.find('img', class_='main-image')
    if main_img:
        analysis['main_image_src'] = main_img.get('src', '')
        analysis['data_images_config'] = main_img.get('data-images', '')

    # è§£ædata-imagesä¸­çš„å›¾ç‰‡
    if analysis['data_images_config']:
        images = [img.strip().replace('../images/products/', '')
                 for img in analysis['data_images_config'].split(',') if img.strip()]

        analysis['total_images_found'] = len(images)

        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        for img in images:
            img_path = os.path.join(images_dir, img)
            if os.path.exists(img_path):
                analysis['existing_images'].append(img)
            else:
                analysis['missing_images'].append(img)

        # æ£€æŸ¥é‡å¤å›¾ç‰‡
        seen = set()
        for img in images:
            if img in seen:
                analysis['duplicate_images'].append(img)
            seen.add(img)

    # æ£€æŸ¥placeholderé—®é¢˜
    if 'placeholder.jpg' in analysis['data_images_config']:
        analysis['placeholder_issues'].append('data-imagesåŒ…å«placeholder.jpg')

    return analysis

def check_javascript_references(content):
    """æ£€æŸ¥JavaScriptå¼•ç”¨å®Œæ•´æ€§"""
    js_check = {
        'required_scripts': [
            'multi-image-gallery.js',
            'product-database.js',
            'modal-components.js',
            'placeholder-randomizer.js'
        ],
        'found_scripts': [],
        'missing_scripts': [],
        'duplicate_scripts': []
    }

    # æŸ¥æ‰¾æ‰€æœ‰scriptæ ‡ç­¾
    script_matches = re.findall(r'<script[^>]*src="([^"]*)"[^>]*>', content)

    for script_src in script_matches:
        script_name = script_src.split('/')[-1]
        js_check['found_scripts'].append(script_name)

    # æ£€æŸ¥å¿…éœ€è„šæœ¬
    for required in js_check['required_scripts']:
        if required not in js_check['found_scripts']:
            js_check['missing_scripts'].append(required)

    # æ£€æŸ¥é‡å¤è„šæœ¬
    seen = set()
    for script in js_check['found_scripts']:
        if script in seen:
            js_check['duplicate_scripts'].append(script)
        seen.add(script)

    return js_check

def analyze_carousel_config(soup, content, images_dir):
    """åˆ†æè½®æ’­é…ç½®å’ŒåŠŸèƒ½"""
    carousel = {
        'has_data_images': False,
        'images_count': 0,
        'should_have_carousel': False,
        'has_carousel_class': False,
        'has_thumbnails_container': False,
        'config_issues': []
    }

    main_img = soup.find('img', class_='main-image')
    if main_img:
        data_images = main_img.get('data-images', '')
        carousel['has_data_images'] = bool(data_images)

        if data_images:
            images = [img.strip() for img in data_images.split(',') if img.strip()]
            carousel['images_count'] = len(images)
            carousel['should_have_carousel'] = carousel['images_count'] > 1

        # æ£€æŸ¥æ˜¯å¦æœ‰data-placeholderå±æ€§
        if main_img.get('data-placeholder') == 'true':
            carousel['config_issues'].append('é…ç½®ä¸ºå ä½ç¬¦äº§å“')

    # æ£€æŸ¥è½®æ’­ç›¸å…³å…ƒç´ 
    carousel['has_carousel_class'] = 'multi-image-gallery' in content
    carousel['has_thumbnails_container'] = bool(soup.find('div', class_='image-thumbnails-container'))

    # æ£€æŸ¥é…ç½®é—®é¢˜
    if carousel['images_count'] == 1 and carousel['has_carousel_class']:
        carousel['config_issues'].append('å•å›¾ç‰‡ä¸åº”å¯ç”¨è½®æ’­')

    if carousel['images_count'] > 1 and not carousel['has_carousel_class']:
        carousel['config_issues'].append('å¤šå›¾ç‰‡åº”å¯ç”¨è½®æ’­')

    return carousel

def check_quote_function(soup, content, product_id):
    """æ£€æŸ¥è·å–æŠ¥ä»·åŠŸèƒ½"""
    quote = {
        'has_quote_button': False,
        'button_onclick': '',
        'product_id_correct': False,
        'has_modal_script': False,
        'potential_image_issues': []
    }

    # æŸ¥æ‰¾è·å–æŠ¥ä»·æŒ‰é’®
    quote_buttons = soup.find_all('button', string=re.compile(r'è·å–æŠ¥ä»·|æŠ¥ä»·'))
    if quote_buttons:
        quote['has_quote_button'] = True
        for btn in quote_buttons:
            onclick = btn.get('onclick', '')
            quote['button_onclick'] = onclick

            # æ£€æŸ¥product_idæ˜¯å¦æ­£ç¡®
            if product_id in onclick:
                quote['product_id_correct'] = True

    # æ£€æŸ¥modalè„šæœ¬
    quote['has_modal_script'] = 'modal-components.js' in content

    # æ£€æŸ¥å¯èƒ½çš„å›¾ç‰‡é—®é¢˜ï¼ˆè·¯å¾„ç›¸å…³ï¼‰
    if '../images/' in content and quote['has_quote_button']:
        # è·å–æŠ¥ä»·åŠŸèƒ½å¯èƒ½ä½¿ç”¨å›¾ç‰‡è·¯å¾„ï¼Œæ£€æŸ¥è·¯å¾„ä¸€è‡´æ€§
        main_img = soup.find('img', class_='main-image')
        if main_img:
            img_src = main_img.get('src', '')
            if img_src.startswith('../images/'):
                quote['potential_image_issues'].append('å›¾ç‰‡è·¯å¾„å¯èƒ½å½±å“æŠ¥ä»·æ˜¾ç¤º')

    return quote

def check_css_references(content):
    """æ£€æŸ¥CSSå¼•ç”¨"""
    css_check = {
        'required_css': [
            'multi-image-gallery.css',
            'product-detail-modern.css',
            'product-placeholder.css'
        ],
        'found_css': [],
        'missing_css': []
    }

    # æŸ¥æ‰¾CSSå¼•ç”¨
    css_matches = re.findall(r'<link[^>]*href="([^"]*\.css)"[^>]*>', content)

    for css_href in css_matches:
        css_name = css_href.split('/')[-1].split('?')[0]  # å»é™¤ç‰ˆæœ¬å·
        css_check['found_css'].append(css_name)

    # æ£€æŸ¥å¿…éœ€CSS
    for required in css_check['required_css']:
        if required not in css_check['found_css']:
            css_check['missing_css'].append(required)

    return css_check

def compile_issues(result):
    """ç¼–è¯‘æ‰€æœ‰å‘ç°çš„é—®é¢˜"""
    issues = []

    # HTMLç»“æ„é—®é¢˜
    html = result['html_structure']
    if not html['has_main_image']:
        issues.append({'type': 'critical', 'category': 'html', 'message': 'ç¼ºå°‘ä¸»å›¾ç‰‡å…ƒç´ '})
    if html['unclosed_tags']:
        issues.append({'type': 'warning', 'category': 'html', 'message': f'å‘ç°{len(html["unclosed_tags"])}ä¸ªæœªé—­åˆçš„imgæ ‡ç­¾'})

    # å›¾ç‰‡é—®é¢˜
    img = result['image_analysis']
    if img['missing_images']:
        issues.append({'type': 'critical', 'category': 'images', 'message': f'ç¼ºå¤±å›¾ç‰‡æ–‡ä»¶: {", ".join(img["missing_images"])}'})
    if img['placeholder_issues']:
        issues.append({'type': 'warning', 'category': 'images', 'message': 'è½®æ’­é…ç½®åŒ…å«placeholder.jpg'})
    if img['duplicate_images']:
        issues.append({'type': 'info', 'category': 'images', 'message': f'é‡å¤å›¾ç‰‡: {", ".join(img["duplicate_images"])}'})

    # JavaScripté—®é¢˜
    js = result['javascript_check']
    if js['missing_scripts']:
        issues.append({'type': 'critical', 'category': 'javascript', 'message': f'ç¼ºå°‘JSæ–‡ä»¶: {", ".join(js["missing_scripts"])}'})

    # è½®æ’­é—®é¢˜
    carousel = result['carousel_analysis']
    if carousel['config_issues']:
        for issue in carousel['config_issues']:
            issues.append({'type': 'warning', 'category': 'carousel', 'message': issue})

    # æŠ¥ä»·åŠŸèƒ½é—®é¢˜
    quote = result['quote_function']
    if not quote['has_quote_button']:
        issues.append({'type': 'warning', 'category': 'quote', 'message': 'ç¼ºå°‘è·å–æŠ¥ä»·æŒ‰é’®'})
    elif not quote['product_id_correct']:
        issues.append({'type': 'critical', 'category': 'quote', 'message': 'æŠ¥ä»·æŒ‰é’®product_idä¸æ­£ç¡®'})
    if quote['potential_image_issues']:
        issues.append({'type': 'warning', 'category': 'quote', 'message': 'æŠ¥ä»·åŠŸèƒ½å¯èƒ½å­˜åœ¨å›¾ç‰‡æ˜¾ç¤ºé—®é¢˜'})

    return issues

def assess_severity(result):
    """è¯„ä¼°é—®é¢˜ä¸¥é‡ç¨‹åº¦å’Œä¿®å¤ä¼˜å…ˆçº§"""
    critical_count = len([i for i in result['issues'] if i['type'] == 'critical'])
    warning_count = len([i for i in result['issues'] if i['type'] == 'warning'])

    if critical_count > 0:
        severity = 'critical'
        priority = 1
    elif warning_count > 2:
        severity = 'high'
        priority = 2
    elif warning_count > 0:
        severity = 'medium'
        priority = 3
    else:
        severity = 'low'
        priority = 4

    return severity, priority

def print_brief_result(product_id, result):
    """æ‰“å°ç®€è¦å®¡è®¡ç»“æœ"""
    severity_icons = {
        'critical': 'ğŸ”´',
        'high': 'ğŸŸ ',
        'medium': 'ğŸŸ¡',
        'low': 'ğŸŸ¢',
        'unknown': 'âšª'
    }

    icon = severity_icons.get(result['severity'], 'âšª')
    issue_count = len(result['issues'])

    print(f"   {icon} {result['severity'].upper()} - {issue_count}ä¸ªé—®é¢˜")

    # æ˜¾ç¤ºæœ€ä¸¥é‡çš„é—®é¢˜
    critical_issues = [i for i in result['issues'] if i['type'] == 'critical']
    if critical_issues:
        print(f"      ğŸš¨ å…³é”®é—®é¢˜: {critical_issues[0]['message']}")

def generate_comprehensive_report(audit_results, base_dir):
    """ç”Ÿæˆè¯¦ç»†å®¡è®¡æŠ¥å‘Š"""
    report_path = os.path.join(base_dir, 'audit-report.json')

    # ç»Ÿè®¡ä¿¡æ¯
    total_products = len(audit_results)
    critical_products = len([r for r in audit_results.values() if r['severity'] == 'critical'])
    high_products = len([r for r in audit_results.values() if r['severity'] == 'high'])

    summary = {
        'audit_summary': {
            'total_products': total_products,
            'critical_issues': critical_products,
            'high_priority': high_products,
            'audit_time': datetime.now().isoformat()
        },
        'detailed_results': audit_results
    }

    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

def generate_fix_plan(audit_results, base_dir):
    """ç”Ÿæˆä¿®å¤è®¡åˆ’"""
    fix_plan_path = os.path.join(base_dir, 'fix-plan.md')

    # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
    priority_groups = {1: [], 2: [], 3: [], 4: []}
    for product_id, result in audit_results.items():
        priority_groups[result['fix_priority']].append((product_id, result))

    with open(fix_plan_path, 'w', encoding='utf-8') as f:
        f.write("# äº§å“é¡µé¢ä¿®å¤è®¡åˆ’\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        priority_names = {1: 'ğŸ”´ å…³é”®ä¼˜å…ˆçº§', 2: 'ğŸŸ  é«˜ä¼˜å…ˆçº§', 3: 'ğŸŸ¡ ä¸­ä¼˜å…ˆçº§', 4: 'ğŸŸ¢ ä½ä¼˜å…ˆçº§'}

        for priority in [1, 2, 3, 4]:
            products = priority_groups[priority]
            if products:
                f.write(f"## {priority_names[priority]} ({len(products)}ä¸ªäº§å“)\n\n")

                for product_id, result in products:
                    f.write(f"### {product_id}\n")
                    f.write(f"- ä¸¥é‡ç¨‹åº¦: {result['severity']}\n")
                    f.write(f"- é—®é¢˜æ•°é‡: {len(result['issues'])}\n")
                    f.write("- é—®é¢˜æ¸…å•:\n")

                    for issue in result['issues']:
                        f.write(f"  - **{issue['type'].upper()}** [{issue['category']}]: {issue['message']}\n")

                    f.write("\n")

        # ä¿®å¤å»ºè®®
        f.write("## ä¿®å¤å»ºè®®\n\n")
        f.write("1. **ç«‹å³å¤„ç†å…³é”®é—®é¢˜**: å½±å“æ ¸å¿ƒåŠŸèƒ½çš„äº§å“\n")
        f.write("2. **æ‰¹é‡ä¿®å¤é…ç½®é—®é¢˜**: JavaScriptå¼•ç”¨ã€å›¾ç‰‡è·¯å¾„ç­‰\n")
        f.write("3. **æµ‹è¯•éªŒè¯**: æ¯ä¿®å¤ä¸€ä¸ªäº§å“ç«‹å³æµ‹è¯•\n")
        f.write("4. **è´¨é‡ä¿è¯**: ç¡®ä¿ä¿®å¤ä¸å¼•å…¥æ–°é—®é¢˜\n")

    print(f"ğŸ“‹ ä¿®å¤è®¡åˆ’å·²ä¿å­˜åˆ°: {fix_plan_path}")

if __name__ == '__main__':
    main()