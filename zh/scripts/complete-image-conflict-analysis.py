#!/usr/bin/env python3
"""
å®Œæ•´çš„å›¾ç‰‡ç³»ç»Ÿå†²çªåˆ†æ - å½»åº•è§£å†³å¤šå›¾ç‰‡åº“å†²çªé—®é¢˜
åˆ†ææ‰€æœ‰HTMLæ–‡ä»¶çš„å›¾ç‰‡å¼•ç”¨ï¼Œå»ºç«‹å®Œæ•´çš„è·¯å¾„æ˜ å°„è¡¨
"""

import os
import glob
import re
import json
from pathlib import Path
from collections import defaultdict

# è·¯å¾„é…ç½®
PROJECT_ROOT = r"D:\ai\æ–°å»ºæ–‡ä»¶å¤¹\æ–°å»ºæ–‡ä»¶å¤¹\7788"
IMAGES_ROOT = os.path.join(PROJECT_ROOT, "images")
PRODUCTS_DIR = os.path.join(PROJECT_ROOT, "products")
SCRIPTS_DIR = os.path.join(PROJECT_ROOT, "scripts")

def scan_all_images():
    """æ‰«ææ‰€æœ‰å›¾ç‰‡æ–‡ä»¶"""
    print("ğŸ” æ‰«ææ‰€æœ‰å›¾ç‰‡æ–‡ä»¶...")

    image_libraries = {
        'images_root': {},
        'images_products': {},
        'backup_images': {}
    }

    # æ‰«ææ ¹ç›®å½•images/
    for ext in ['*.png', '*.jpg', '*.jpeg']:
        for img_path in glob.glob(os.path.join(IMAGES_ROOT, ext)):
            filename = os.path.basename(img_path)
            rel_path = os.path.relpath(img_path, PROJECT_ROOT).replace('\\', '/')
            image_libraries['images_root'][filename] = {
                'path': rel_path,
                'size': os.path.getsize(img_path),
                'exists': True
            }

    # æ‰«æimages/products/
    products_img_dir = os.path.join(IMAGES_ROOT, "products")
    if os.path.exists(products_img_dir):
        for ext in ['*.png', '*.jpg', '*.jpeg']:
            for img_path in glob.glob(os.path.join(products_img_dir, ext)):
                filename = os.path.basename(img_path)
                rel_path = os.path.relpath(img_path, PROJECT_ROOT).replace('\\', '/')
                image_libraries['images_products'][filename] = {
                    'path': rel_path,
                    'size': os.path.getsize(img_path),
                    'exists': True
                }

    # æ‰«æå¤‡ä»½ç›®å½•
    backup_dir = os.path.join(SCRIPTS_DIR, "image_backup")
    if os.path.exists(backup_dir):
        for ext in ['*.png', '*.jpg', '*.jpeg']:
            for img_path in glob.glob(os.path.join(backup_dir, ext)):
                filename = os.path.basename(img_path)
                rel_path = os.path.relpath(img_path, PROJECT_ROOT).replace('\\', '/')
                image_libraries['backup_images'][filename] = {
                    'path': rel_path,
                    'size': os.path.getsize(img_path),
                    'exists': True
                }

    print(f"   ğŸ“ æ ¹ç›®å½•å›¾ç‰‡: {len(image_libraries['images_root'])} ä¸ª")
    print(f"   ğŸ“ productsç›®å½•å›¾ç‰‡: {len(image_libraries['images_products'])} ä¸ª")
    print(f"   ğŸ“ å¤‡ä»½ç›®å½•å›¾ç‰‡: {len(image_libraries['backup_images'])} ä¸ª")

    return image_libraries

def scan_html_image_references():
    """æ‰«ææ‰€æœ‰HTMLæ–‡ä»¶ä¸­çš„å›¾ç‰‡å¼•ç”¨"""
    print("ğŸ” æ‰«æHTMLæ–‡ä»¶ä¸­çš„å›¾ç‰‡å¼•ç”¨...")

    html_references = {}

    # æ‰«æä¸»è¦HTMLæ–‡ä»¶
    html_files = [
        'products.html',
        'index.html',
        'about.html',
        'contact.html',
        'quality.html',
        'applications.html'
    ]

    for html_file in html_files:
        html_path = os.path.join(PROJECT_ROOT, html_file)
        if os.path.exists(html_path):
            html_references[html_file] = analyze_html_images(html_path)

    # æ‰«æäº§å“è¯¦æƒ…é¡µ
    product_files = glob.glob(os.path.join(PRODUCTS_DIR, "*.html"))
    for product_file in product_files:
        filename = os.path.basename(product_file)
        html_references[f"products/{filename}"] = analyze_html_images(product_file)

    return html_references

def analyze_html_images(html_path):
    """åˆ†æå•ä¸ªHTMLæ–‡ä»¶çš„å›¾ç‰‡å¼•ç”¨"""
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # æŸ¥æ‰¾æ‰€æœ‰imgæ ‡ç­¾
        img_pattern = r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
        img_matches = re.findall(img_pattern, content)

        # æŸ¥æ‰¾CSSä¸­çš„èƒŒæ™¯å›¾ç‰‡
        css_bg_pattern = r'background-image:\s*url\(["\']?([^"\')\s]+)["\']?\)'
        css_matches = re.findall(css_bg_pattern, content)

        # æŸ¥æ‰¾data-imageså±æ€§
        data_images_pattern = r'data-images=["\']([^"\']+)["\']'
        data_matches = re.findall(data_images_pattern, content)

        references = {
            'img_src': img_matches,
            'css_backgrounds': css_matches,
            'data_images': []
        }

        # å¤„ç†data-imagesï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªå›¾ç‰‡ï¼‰
        for data_imgs in data_matches:
            images = [img.strip() for img in data_imgs.split(',') if img.strip()]
            references['data_images'].extend(images)

        return references

    except Exception as e:
        print(f"   âŒ è¯»å– {html_path} å¤±è´¥: {e}")
        return {'img_src': [], 'css_backgrounds': [], 'data_images': []}

def analyze_conflicts(image_libraries, html_references):
    """åˆ†æå›¾ç‰‡å†²çªå’Œç¼ºå¤±"""
    print("ğŸ” åˆ†æå›¾ç‰‡å†²çªå’Œç¼ºå¤±...")

    analysis = {
        'missing_images': [],
        'conflicting_paths': [],
        'correct_mappings': {},
        'orphaned_images': []
    }

    # æ”¶é›†æ‰€æœ‰è¢«å¼•ç”¨çš„å›¾ç‰‡è·¯å¾„
    all_referenced_paths = set()

    for html_file, refs in html_references.items():
        for img_list in refs.values():
            for img_path in img_list:
                if img_path.startswith('images/'):
                    all_referenced_paths.add(img_path)

    print(f"   ğŸ“Š å‘ç° {len(all_referenced_paths)} ä¸ªä¸åŒçš„å›¾ç‰‡å¼•ç”¨")

    # åˆ†ææ¯ä¸ªå¼•ç”¨çš„å›¾ç‰‡
    for ref_path in all_referenced_paths:
        filename = os.path.basename(ref_path)
        full_path = os.path.join(PROJECT_ROOT, ref_path.replace('/', '\\'))

        if os.path.exists(full_path):
            # å›¾ç‰‡å­˜åœ¨ï¼Œè®°å½•æ­£ç¡®æ˜ å°„
            analysis['correct_mappings'][ref_path] = ref_path
        else:
            # å›¾ç‰‡ä¸å­˜åœ¨ï¼ŒæŸ¥æ‰¾å¯èƒ½çš„æ›¿ä»£å“
            analysis['missing_images'].append({
                'referenced_path': ref_path,
                'filename': filename,
                'potential_matches': find_potential_matches(filename, image_libraries)
            })

    # æŸ¥æ‰¾å­¤ç«‹çš„å›¾ç‰‡ï¼ˆå­˜åœ¨ä½†æœªè¢«å¼•ç”¨ï¼‰
    all_existing_images = set()

    for lib_name, lib_images in image_libraries.items():
        for img_name, img_info in lib_images.items():
            all_existing_images.add(img_info['path'])

    referenced_paths_set = set(analysis['correct_mappings'].keys())
    orphaned = all_existing_images - referenced_paths_set

    for orphan in orphaned:
        if 'images/' in orphan:  # åªå…³æ³¨imagesç›®å½•ä¸‹çš„å›¾ç‰‡
            analysis['orphaned_images'].append(orphan)

    return analysis

def find_potential_matches(missing_filename, image_libraries):
    """æŸ¥æ‰¾å¯èƒ½çš„å›¾ç‰‡åŒ¹é…"""
    potential_matches = []

    # æå–äº§å“åç§°ç”¨äºæ¨¡ç³ŠåŒ¹é…
    missing_base = missing_filename.lower().replace('.png', '').replace('.jpg', '').replace('.jpeg', '')
    missing_parts = re.split(r'[-_]', missing_base)

    # åœ¨æ‰€æœ‰å›¾ç‰‡åº“ä¸­æŸ¥æ‰¾åŒ¹é…
    for lib_name, lib_images in image_libraries.items():
        for img_name, img_info in lib_images.items():
            img_base = img_name.lower().replace('.png', '').replace('.jpg', '').replace('.jpeg', '')
            img_parts = re.split(r'[-_]', img_base)

            # è®¡ç®—åŒ¹é…åº¦
            common_parts = set(missing_parts) & set(img_parts)
            if len(common_parts) >= 2 or (len(common_parts) >= 1 and len(missing_parts) <= 2):
                potential_matches.append({
                    'library': lib_name,
                    'filename': img_name,
                    'path': img_info['path'],
                    'confidence': len(common_parts) / max(len(missing_parts), len(img_parts)),
                    'size': img_info['size']
                })

    # æŒ‰ç½®ä¿¡åº¦æ’åº
    potential_matches.sort(key=lambda x: x['confidence'], reverse=True)
    return potential_matches[:5]  # è¿”å›å‰5ä¸ªæœ€ä½³åŒ¹é…

def generate_fix_recommendations(analysis):
    """ç”Ÿæˆä¿®å¤å»ºè®®"""
    print("ğŸ’¡ ç”Ÿæˆä¿®å¤å»ºè®®...")

    recommendations = {
        'path_replacements': {},
        'image_migrations': [],
        'cleanup_suggestions': []
    }

    # ä¸ºç¼ºå¤±çš„å›¾ç‰‡ç”Ÿæˆè·¯å¾„æ›¿æ¢å»ºè®®
    for missing in analysis['missing_images']:
        ref_path = missing['referenced_path']
        potential_matches = missing['potential_matches']

        if potential_matches:
            best_match = potential_matches[0]
            if best_match['confidence'] > 0.4:  # ç½®ä¿¡åº¦é˜ˆå€¼
                recommendations['path_replacements'][ref_path] = {
                    'original': ref_path,
                    'replacement': best_match['path'],
                    'confidence': best_match['confidence'],
                    'reason': f"ä»{best_match['library']}æ‰¾åˆ°æœ€ä½³åŒ¹é…"
                }

    # å»ºè®®å›¾ç‰‡è¿ç§»ï¼ˆä»æ ¹ç›®å½•ç§»åŠ¨åˆ°productsç›®å½•ï¼‰
    for lib_name, lib_images in analysis.get('image_libraries', {}).items():
        if lib_name == 'images_root':
            for img_name, img_info in lib_images.items():
                if any(keyword in img_name.lower() for keyword in
                      ['brick', 'alumina', 'clay', 'silica', 'mullite', 'castable']):
                    recommendations['image_migrations'].append({
                        'source': img_info['path'],
                        'target': f"images/products/{img_name}",
                        'reason': "äº§å“å›¾ç‰‡åº”ç»Ÿä¸€æ”¾åœ¨productsç›®å½•"
                    })

    return recommendations

def generate_comprehensive_report(image_libraries, html_references, analysis, recommendations):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    print("ğŸ“Š ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")

    # ç»Ÿè®¡æ•°æ®
    total_images = sum(len(lib) for lib in image_libraries.values())
    total_references = sum(
        len(refs['img_src']) + len(refs['css_backgrounds']) + len(refs['data_images'])
        for refs in html_references.values()
    )
    missing_count = len(analysis['missing_images'])
    orphaned_count = len(analysis['orphaned_images'])

    print("=" * 80)
    print("ğŸ“‹ å›¾ç‰‡ç³»ç»Ÿå†²çªåˆ†ææŠ¥å‘Š")
    print("=" * 80)

    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   å›¾ç‰‡åº“æ–‡ä»¶æ€»æ•°: {total_images}")
    print(f"   HTMLå¼•ç”¨æ€»æ•°: {total_references}")
    print(f"   ç¼ºå¤±å›¾ç‰‡æ•°é‡: {missing_count}")
    print(f"   å­¤ç«‹å›¾ç‰‡æ•°é‡: {orphaned_count}")
    print(f"   å¯ä¿®å¤å¼•ç”¨: {len(recommendations['path_replacements'])}")

    print(f"\nğŸ—ï¸ å›¾ç‰‡åº“åˆ†å¸ƒ:")
    for lib_name, lib_images in image_libraries.items():
        print(f"   {lib_name}: {len(lib_images)} ä¸ªæ–‡ä»¶")

    print(f"\nâŒ ç¼ºå¤±å›¾ç‰‡è¯¦æƒ… (å‰10ä¸ª):")
    for i, missing in enumerate(analysis['missing_images'][:10], 1):
        ref_path = missing['referenced_path']
        potential = missing['potential_matches']
        print(f"   {i:2d}. {ref_path}")
        if potential:
            best = potential[0]
            print(f"       â†’ å»ºè®®æ›¿æ¢ä¸º: {best['path']} (ç½®ä¿¡åº¦: {best['confidence']:.2f})")
        else:
            print(f"       â†’ æ— åˆé€‚æ›¿ä»£å“")

    if len(analysis['missing_images']) > 10:
        print(f"   ... è¿˜æœ‰ {len(analysis['missing_images']) - 10} ä¸ªç¼ºå¤±å›¾ç‰‡")

    print(f"\nâœ… å¯è‡ªåŠ¨ä¿®å¤çš„è·¯å¾„ (å‰10ä¸ª):")
    count = 0
    for original, replacement in recommendations['path_replacements'].items():
        if count >= 10:
            break
        print(f"   {count+1:2d}. {original}")
        print(f"       â†’ {replacement['replacement']} (ç½®ä¿¡åº¦: {replacement['confidence']:.2f})")
        count += 1

    if len(recommendations['path_replacements']) > 10:
        print(f"   ... è¿˜æœ‰ {len(recommendations['path_replacements']) - 10} ä¸ªå¯ä¿®å¤è·¯å¾„")

    print(f"\nğŸ”§ ä¿®å¤ä¼˜å…ˆçº§å»ºè®®:")
    print(f"   ğŸš¨ ç«‹å³ä¿®å¤: products.html ä¸­çš„å›¾ç‰‡å¼•ç”¨ ({missing_count} ä¸ª)")
    print(f"   âš™ï¸ ä¸­æœŸæ•´ç†: å›¾ç‰‡åº“ç»Ÿä¸€è¿ç§» ({len(recommendations['image_migrations'])} ä¸ª)")
    print(f"   ğŸ§¹ é•¿æœŸç»´æŠ¤: æ¸…ç†å­¤ç«‹å›¾ç‰‡ ({orphaned_count} ä¸ª)")

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_data = {
        'timestamp': str(Path().absolute()),
        'statistics': {
            'total_images': total_images,
            'total_references': total_references,
            'missing_count': missing_count,
            'orphaned_count': orphaned_count
        },
        'image_libraries': image_libraries,
        'html_references': html_references,
        'analysis': analysis,
        'recommendations': recommendations
    }

    report_file = os.path.join(SCRIPTS_DIR, 'complete_image_conflict_report.json')
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    return report_data

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ” å®Œæ•´å›¾ç‰‡ç³»ç»Ÿå†²çªåˆ†æ")
    print("=" * 80)

    # æ‰«æå›¾ç‰‡åº“
    image_libraries = scan_all_images()

    # æ‰«æHTMLå¼•ç”¨
    html_references = scan_html_image_references()

    # åˆ†æå†²çª
    analysis = analyze_conflicts(image_libraries, html_references)

    # ç”Ÿæˆä¿®å¤å»ºè®®
    recommendations = generate_fix_recommendations(analysis)

    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    report_data = generate_comprehensive_report(
        image_libraries, html_references, analysis, recommendations
    )

    return report_data

if __name__ == "__main__":
    main()