#!/usr/bin/env python3
"""
æœ€ç»ˆå›¾ç‰‡ä¸€è‡´æ€§æ£€æŸ¥å·¥å…·
å‡†ç¡®æ£€æŸ¥æ‰€æœ‰HTMLæ–‡ä»¶ä¸­çš„å›¾ç‰‡å¼•ç”¨å’Œå®é™…æ–‡ä»¶å­˜åœ¨æƒ…å†µ
ç”Ÿæˆå®Œæ•´çš„å›¾ç‰‡ç³»ç»Ÿå¥åº·æŠ¥å‘Š
"""

import os
import glob
import re
import json
from pathlib import Path

# è·¯å¾„é…ç½®
PROJECT_ROOT = r"D:\ai\æ–°å»ºæ–‡ä»¶å¤¹\æ–°å»ºæ–‡ä»¶å¤¹\7788"
PRODUCTS_DIR = os.path.join(PROJECT_ROOT, "products")
IMAGES_ROOT = os.path.join(PROJECT_ROOT, "images")
SCRIPTS_DIR = os.path.join(PROJECT_ROOT, "scripts")

def scan_all_html_files():
    """æ‰«ææ‰€æœ‰HTMLæ–‡ä»¶"""
    html_files = []

    # ä¸»è¦é¡µé¢
    main_pages = ['products.html', 'index.html', 'about.html', 'contact.html', 'quality.html', 'applications.html']
    for page in main_pages:
        page_path = os.path.join(PROJECT_ROOT, page)
        if os.path.exists(page_path):
            html_files.append({
                'type': 'main_page',
                'file': page,
                'path': page_path
            })

    # äº§å“è¯¦æƒ…é¡µ
    for product_file in glob.glob(os.path.join(PRODUCTS_DIR, "*.html")):
        html_files.append({
            'type': 'product_page',
            'file': f"products/{os.path.basename(product_file)}",
            'path': product_file
        })

    return html_files

def extract_all_image_references(html_path):
    """æå–HTMLæ–‡ä»¶ä¸­çš„æ‰€æœ‰å›¾ç‰‡å¼•ç”¨"""
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return {'error': str(e), 'references': []}

    references = []

    # 1. imgæ ‡ç­¾çš„srcå±æ€§ (æ”¯æŒå¤šè¡Œ)
    img_pattern = r'<img[^>]*src\s*=\s*["\']([^"\']+)["\'][^>]*>'
    img_matches = re.findall(img_pattern, content, re.DOTALL | re.IGNORECASE)
    for match in img_matches:
        references.append({
            'type': 'img_src',
            'path': match.strip(),
            'tag': 'img'
        })

    # 2. data-imageså±æ€§ (è½®æ’­å›¾ç‰‡)
    data_images_pattern = r'data-images\s*=\s*["\']([^"\']+)["\']'
    data_matches = re.findall(data_images_pattern, content, re.DOTALL | re.IGNORECASE)
    for match in data_matches:
        for img_path in match.split(','):
            img_path = img_path.strip()
            if img_path:
                references.append({
                    'type': 'data_images',
                    'path': img_path,
                    'tag': 'carousel'
                })

    # 3. CSSèƒŒæ™¯å›¾ç‰‡
    css_bg_pattern = r'background-image\s*:\s*url\s*\(\s*["\']?([^"\')\s]+)["\']?\s*\)'
    css_matches = re.findall(css_bg_pattern, content, re.DOTALL | re.IGNORECASE)
    for match in css_matches:
        references.append({
            'type': 'css_background',
            'path': match.strip(),
            'tag': 'css'
        })

    # 4. å†…è”æ ·å¼èƒŒæ™¯å›¾ç‰‡
    inline_bg_pattern = r'style\s*=\s*["\'][^"\']*background-image\s*:\s*url\s*\(\s*["\']?([^"\')\s]+)["\']?\s*\)[^"\']*["\']'
    inline_matches = re.findall(inline_bg_pattern, content, re.DOTALL | re.IGNORECASE)
    for match in inline_matches:
        references.append({
            'type': 'inline_background',
            'path': match.strip(),
            'tag': 'style'
        })

    return {'error': None, 'references': references}

def check_file_existence(img_path, base_path):
    """æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    # å¤„ç†ç›¸å¯¹è·¯å¾„
    if img_path.startswith('../'):
        # ä»äº§å“é¡µé¢å¼•ç”¨çš„ç›¸å¯¹è·¯å¾„
        full_path = os.path.join(os.path.dirname(base_path), img_path.replace('../', ''))
    elif img_path.startswith('./'):
        # å½“å‰ç›®å½•ç›¸å¯¹è·¯å¾„
        full_path = os.path.join(os.path.dirname(base_path), img_path[2:])
    elif not img_path.startswith('/') and not img_path.startswith('http'):
        # ç›¸å¯¹è·¯å¾„ï¼ˆä»å½“å‰æ–‡ä»¶ä½ç½®ï¼‰
        full_path = os.path.join(os.path.dirname(base_path), img_path)
    else:
        # ç»å¯¹è·¯å¾„æˆ–URL
        if img_path.startswith('http'):
            return {'exists': True, 'type': 'external_url', 'path': img_path}
        full_path = img_path

    # æ ‡å‡†åŒ–è·¯å¾„
    full_path = os.path.normpath(full_path)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    exists = os.path.exists(full_path)

    result = {
        'exists': exists,
        'type': 'local_file',
        'full_path': full_path,
        'path': img_path
    }

    if exists:
        result['size'] = os.path.getsize(full_path)

    return result

def analyze_image_consistency():
    """åˆ†æå›¾ç‰‡ä¸€è‡´æ€§"""
    print("ğŸ” å¼€å§‹æœ€ç»ˆå›¾ç‰‡ä¸€è‡´æ€§æ£€æŸ¥...")

    html_files = scan_all_html_files()
    print(f"ğŸ“„ å‘ç° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")

    analysis_results = {
        'html_files': {},
        'summary': {
            'total_files': len(html_files),
            'total_references': 0,
            'valid_references': 0,
            'invalid_references': 0,
            'external_references': 0
        },
        'issues': []
    }

    for html_file in html_files:
        file_key = html_file['file']
        print(f"   åˆ†æ: {file_key}")

        # æå–å›¾ç‰‡å¼•ç”¨
        extraction = extract_all_image_references(html_file['path'])

        if extraction['error']:
            analysis_results['html_files'][file_key] = {
                'error': extraction['error'],
                'status': 'read_error'
            }
            continue

        references = extraction['references']
        file_analysis = {
            'type': html_file['type'],
            'total_references': len(references),
            'valid_references': [],
            'invalid_references': [],
            'external_references': [],
            'status': 'analyzed'
        }

        # æ£€æŸ¥æ¯ä¸ªå›¾ç‰‡å¼•ç”¨
        for ref in references:
            check_result = check_file_existence(ref['path'], html_file['path'])

            ref_info = {
                'type': ref['type'],
                'tag': ref['tag'],
                'path': ref['path'],
                'exists': check_result['exists'],
                'full_path': check_result.get('full_path', ''),
                'size': check_result.get('size', 0)
            }

            if check_result['type'] == 'external_url':
                file_analysis['external_references'].append(ref_info)
                analysis_results['summary']['external_references'] += 1
            elif check_result['exists']:
                file_analysis['valid_references'].append(ref_info)
                analysis_results['summary']['valid_references'] += 1
            else:
                file_analysis['invalid_references'].append(ref_info)
                analysis_results['summary']['invalid_references'] += 1

                # è®°å½•é—®é¢˜
                analysis_results['issues'].append({
                    'file': file_key,
                    'type': 'missing_image',
                    'reference_type': ref['type'],
                    'path': ref['path'],
                    'full_path': check_result.get('full_path', '')
                })

        analysis_results['summary']['total_references'] += len(references)
        analysis_results['html_files'][file_key] = file_analysis

    return analysis_results

def find_orphaned_images():
    """æŸ¥æ‰¾å­¤ç«‹çš„å›¾ç‰‡æ–‡ä»¶"""
    print("ğŸ” æŸ¥æ‰¾å­¤ç«‹çš„å›¾ç‰‡æ–‡ä»¶...")

    # æ”¶é›†æ‰€æœ‰è¢«å¼•ç”¨çš„å›¾ç‰‡è·¯å¾„
    analysis = analyze_image_consistency()
    referenced_images = set()

    for file_key, file_data in analysis['html_files'].items():
        if file_data.get('status') == 'analyzed':
            for ref in file_data['valid_references']:
                full_path = os.path.normpath(ref['full_path'])
                referenced_images.add(full_path)

    # æ‰«ææ‰€æœ‰å®é™…å­˜åœ¨çš„å›¾ç‰‡æ–‡ä»¶
    all_images = set()
    for ext in ['*.png', '*.jpg', '*.jpeg', '*.gif', '*.svg', '*.webp']:
        for img_path in glob.glob(os.path.join(IMAGES_ROOT, "**", ext), recursive=True):
            all_images.add(os.path.normpath(img_path))

    # æŸ¥æ‰¾å­¤ç«‹æ–‡ä»¶
    orphaned_images = all_images - referenced_images

    # è¿‡æ»¤æ‰ä¸€äº›ç‰¹æ®Šç›®å½•ï¼ˆå¦‚å¤‡ä»½ç›®å½•ï¼‰
    filtered_orphaned = []
    for orphan in orphaned_images:
        rel_path = os.path.relpath(orphan, PROJECT_ROOT)
        if not any(skip in rel_path for skip in ['backup', 'temp', 'cache', '.git']):
            filtered_orphaned.append({
                'path': orphan,
                'relative_path': rel_path,
                'size': os.path.getsize(orphan)
            })

    return filtered_orphaned

def generate_final_report(analysis, orphaned_images):
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æœ€ç»ˆå›¾ç‰‡ä¸€è‡´æ€§æ£€æŸ¥æŠ¥å‘Š")
    print("=" * 80)

    # æ€»ä½“ç»Ÿè®¡
    summary = analysis['summary']
    total_refs = summary['total_references']
    valid_refs = summary['valid_references']
    invalid_refs = summary['invalid_references']
    external_refs = summary['external_references']

    success_rate = (valid_refs / total_refs * 100) if total_refs > 0 else 100

    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"   HTMLæ–‡ä»¶: {summary['total_files']} ä¸ª")
    print(f"   å›¾ç‰‡å¼•ç”¨: {total_refs} ä¸ª")
    print(f"   âœ… æœ‰æ•ˆå¼•ç”¨: {valid_refs} ({valid_refs/total_refs*100:.1f}%)")
    print(f"   âŒ æ— æ•ˆå¼•ç”¨: {invalid_refs} ({invalid_refs/total_refs*100:.1f}%)")
    print(f"   ğŸŒ å¤–éƒ¨å¼•ç”¨: {external_refs} ({external_refs/total_refs*100:.1f}%)")
    print(f"   ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")

    # æŒ‰æ–‡ä»¶ç±»å‹åˆ†æ
    main_page_issues = 0
    product_page_issues = 0

    for file_key, file_data in analysis['html_files'].items():
        if file_data.get('status') == 'analyzed':
            invalid_count = len(file_data['invalid_references'])
            if file_data['type'] == 'main_page':
                main_page_issues += invalid_count
            else:
                product_page_issues += invalid_count

    print(f"\nğŸ“„ æŒ‰é¡µé¢ç±»å‹ç»Ÿè®¡:")
    print(f"   ä¸»é¡µé¢é—®é¢˜: {main_page_issues} ä¸ª")
    print(f"   äº§å“é¡µé¢é—®é¢˜: {product_page_issues} ä¸ª")

    # é—®é¢˜è¯¦æƒ…
    if analysis['issues']:
        print(f"\nâŒ æ— æ•ˆå¼•ç”¨è¯¦æƒ… (å‰10ä¸ª):")
        for i, issue in enumerate(analysis['issues'][:10], 1):
            print(f"   {i:2d}. {issue['file']}: {issue['path']}")
            print(f"       ç±»å‹: {issue['reference_type']}")

        if len(analysis['issues']) > 10:
            print(f"   ... è¿˜æœ‰ {len(analysis['issues']) - 10} ä¸ªé—®é¢˜")

    # å­¤ç«‹æ–‡ä»¶æŠ¥å‘Š
    if orphaned_images:
        total_orphaned_size = sum(img['size'] for img in orphaned_images)
        print(f"\nğŸ—‘ï¸  å­¤ç«‹æ–‡ä»¶:")
        print(f"   æ–‡ä»¶æ•°é‡: {len(orphaned_images)} ä¸ª")
        print(f"   æ€»å¤§å°: {total_orphaned_size / 1024 / 1024:.1f} MB")

        if len(orphaned_images) <= 10:
            for orphan in orphaned_images:
                size_mb = orphan['size'] / 1024 / 1024
                print(f"   â€¢ {orphan['relative_path']} ({size_mb:.1f} MB)")
        else:
            for orphan in orphaned_images[:5]:
                size_mb = orphan['size'] / 1024 / 1024
                print(f"   â€¢ {orphan['relative_path']} ({size_mb:.1f} MB)")
            print(f"   ... è¿˜æœ‰ {len(orphaned_images) - 5} ä¸ªå­¤ç«‹æ–‡ä»¶")

    # å¥åº·è¯„ä¼°
    print(f"\nğŸ¥ ç³»ç»Ÿå¥åº·è¯„ä¼°:")
    if success_rate >= 95:
        print(f"   âœ… ä¼˜ç§€ - å›¾ç‰‡ç³»ç»Ÿéå¸¸å¥åº·")
    elif success_rate >= 85:
        print(f"   ğŸŸ¡ è‰¯å¥½ - æœ‰å°‘é‡é—®é¢˜éœ€è¦ä¿®å¤")
    elif success_rate >= 70:
        print(f"   ğŸŸ  ä¸€èˆ¬ - å­˜åœ¨ä¸€äº›é—®é¢˜éœ€è¦å…³æ³¨")
    else:
        print(f"   ğŸ”´ éœ€è¦æ”¹è¿› - å›¾ç‰‡ç³»ç»Ÿéœ€è¦å¤§å¹…ä¼˜åŒ–")

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    final_report = {
        'timestamp': str(Path().absolute()),
        'summary': summary,
        'success_rate': success_rate,
        'analysis': analysis,
        'orphaned_images': orphaned_images,
        'health_status': 'excellent' if success_rate >= 95 else
                       'good' if success_rate >= 85 else
                       'fair' if success_rate >= 70 else 'needs_improvement'
    }

    report_file = os.path.join(SCRIPTS_DIR, 'final_image_consistency_report.json')
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šä¿å­˜åˆ°: {report_file}")

    return final_report

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ” æœ€ç»ˆå›¾ç‰‡ä¸€è‡´æ€§æ£€æŸ¥")
    print("=" * 80)

    # 1. åˆ†æå›¾ç‰‡ä¸€è‡´æ€§
    analysis = analyze_image_consistency()

    # 2. æŸ¥æ‰¾å­¤ç«‹æ–‡ä»¶
    orphaned_images = find_orphaned_images()

    # 3. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    final_report = generate_final_report(analysis, orphaned_images)

    print(f"\nğŸ¯ æ£€æŸ¥å®Œæˆ!")
    print(f"   å›¾ç‰‡å¼•ç”¨æˆåŠŸç‡: {final_report['success_rate']:.1f}%")
    print(f"   ç³»ç»Ÿå¥åº·çŠ¶æ€: {final_report['health_status']}")

    if final_report['success_rate'] >= 95:
        print(f"\nğŸ‰ æ­å–œï¼å›¾ç‰‡ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
    else:
        print(f"\nğŸ“ å»ºè®®:")
        print(f"   1. ä¿®å¤æ— æ•ˆçš„å›¾ç‰‡å¼•ç”¨")
        print(f"   2. æ¸…ç†å­¤ç«‹çš„å›¾ç‰‡æ–‡ä»¶")
        print(f"   3. å»ºç«‹å›¾ç‰‡ç®¡ç†è§„èŒƒ")

if __name__ == "__main__":
    main()