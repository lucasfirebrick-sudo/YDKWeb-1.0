#!/usr/bin/env python3
"""
æ¸…ç†é‡å¤å’Œé”™è¯¯å›¾ç‰‡ - è§£å†³äº§å“å›¾ç‰‡æ··ä¹±é—®é¢˜
åŸºäºè¯Šæ–­ç»“æœè¯†åˆ«å¹¶æ¸…ç†é‡å¤å›¾ç‰‡ï¼Œä¿ç•™æœ€ä½³è´¨é‡ç‰ˆæœ¬
"""

import os
import glob
import re
import json
import shutil
from pathlib import Path
from collections import defaultdict
import hashlib

# è·¯å¾„é…ç½®
PRODUCTS_DIR = r"D:\ai\æ–°å»ºæ–‡ä»¶å¤¹\æ–°å»ºæ–‡ä»¶å¤¹\7788\products"
IMAGES_DIR = r"D:\ai\æ–°å»ºæ–‡ä»¶å¤¹\æ–°å»ºæ–‡ä»¶å¤¹\7788\images\products"
SCRIPTS_DIR = r"D:\ai\æ–°å»ºæ–‡ä»¶å¤¹\æ–°å»ºæ–‡ä»¶å¤¹\7788\scripts"

def calculate_file_hash(file_path):
    """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ç”¨äºè¯†åˆ«é‡å¤æ–‡ä»¶"""
    try:
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except:
        return None

def get_image_priority_score(filename):
    """ä¸ºå›¾ç‰‡æ–‡ä»¶åˆ†é…ä¼˜å…ˆçº§åˆ†æ•°ï¼ˆåˆ†æ•°è¶Šä½ä¼˜å…ˆçº§è¶Šé«˜ï¼‰"""
    filename_lower = filename.lower()

    # å®˜æ–¹å›¾ç‰‡ä¼˜å…ˆçº§æœ€é«˜
    if 'official' in filename_lower:
        return 1

    # æ–°å›¾ç‰‡ä¼˜å…ˆçº§è¾ƒé«˜
    if 'new' in filename_lower:
        return 2

    # ç‰¹å®šäº§å“åç§°å›¾ç‰‡
    if any(name in filename_lower for name in ['brick', 'mullite', 'silica', 'clay', 'alumina']):
        return 3

    # é€šç”¨äº§å“å›¾ç‰‡
    return 4

def analyze_duplicate_images():
    """åˆ†æé‡å¤å›¾ç‰‡"""
    print("ğŸ” åˆ†æå›¾ç‰‡ç›®å½•ä¸­çš„é‡å¤æ–‡ä»¶...")

    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    image_files = []
    for ext in ['*.png', '*.jpg', '*.jpeg']:
        image_files.extend(glob.glob(os.path.join(IMAGES_DIR, ext)))

    print(f"ğŸ“ æ‰¾åˆ° {len(image_files)} ä¸ªå›¾ç‰‡æ–‡ä»¶")

    # æŒ‰å“ˆå¸Œå€¼åˆ†ç»„
    hash_groups = defaultdict(list)
    file_sizes = {}

    for img_path in image_files:
        filename = os.path.basename(img_path)
        file_hash = calculate_file_hash(img_path)

        if file_hash:
            hash_groups[file_hash].append(img_path)
            file_sizes[img_path] = os.path.getsize(img_path)

    # è¯†åˆ«é‡å¤æ–‡ä»¶ç»„
    duplicate_groups = {k: v for k, v in hash_groups.items() if len(v) > 1}

    print(f"ğŸ”„ å‘ç° {len(duplicate_groups)} ç»„é‡å¤æ–‡ä»¶")

    return duplicate_groups, file_sizes

def identify_product_image_conflicts():
    """è¯†åˆ«äº§å“å›¾ç‰‡å†²çª"""
    print("ğŸ” åˆ†æäº§å“å›¾ç‰‡é…ç½®å†²çª...")

    # æŒ‰äº§å“IDåˆ†ç»„å›¾ç‰‡
    product_images = defaultdict(list)

    image_files = glob.glob(os.path.join(IMAGES_DIR, "*"))

    for img_path in image_files:
        filename = os.path.basename(img_path)
        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            # æå–äº§å“ID
            product_id = None

            # å¸¸è§çš„äº§å“IDæ¨¡å¼
            patterns = [
                r'^([a-z-]+)-',  # product-name-xxx
                r'^([a-z_]+)[-_]',  # product_name_xxx
                r'^([a-z]+)-brick',  # xxx-brick
            ]

            for pattern in patterns:
                match = re.match(pattern, filename.lower())
                if match:
                    product_id = match.group(1)
                    break

            if product_id:
                product_images[product_id].append(img_path)

    # è¯†åˆ«æœ‰å¤šä¸ªç‰ˆæœ¬çš„äº§å“
    conflicted_products = {k: v for k, v in product_images.items() if len(v) > 1}

    print(f"âš ï¸  å‘ç° {len(conflicted_products)} ä¸ªäº§å“æœ‰å¤šä¸ªå›¾ç‰‡ç‰ˆæœ¬")

    return conflicted_products

def load_current_configurations():
    """åŠ è½½å½“å‰çš„å›¾ç‰‡é…ç½®"""
    print("ğŸ“– åŠ è½½å½“å‰äº§å“é¡µé¢çš„å›¾ç‰‡é…ç½®...")

    configurations = {}

    for html_file in glob.glob(os.path.join(PRODUCTS_DIR, "*.html")):
        product_id = Path(html_file).stem

        try:
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # æå–data-imagesé…ç½®
            data_images_match = re.search(r'data-images="([^"]*)"', content)
            if data_images_match:
                images = [img.strip() for img in data_images_match.group(1).split(',') if img.strip()]
                # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                abs_images = []
                for img in images:
                    if img.startswith('../images/products/'):
                        abs_path = os.path.join(IMAGES_DIR, img.replace('../images/products/', ''))
                        abs_images.append(abs_path)

                configurations[product_id] = abs_images

        except Exception as e:
            print(f"âŒ è¯»å– {product_id} é…ç½®å¤±è´¥: {e}")

    return configurations

def create_cleanup_plan(duplicate_groups, conflicted_products, current_configs, file_sizes):
    """åˆ›å»ºæ¸…ç†è®¡åˆ’"""
    print("ğŸ“‹ åˆ›å»ºå›¾ç‰‡æ¸…ç†è®¡åˆ’...")

    cleanup_plan = {
        'duplicate_removals': [],
        'conflict_resolutions': [],
        'orphaned_files': [],
        'config_updates': {}
    }

    # å¤„ç†é‡å¤æ–‡ä»¶
    for file_hash, file_list in duplicate_groups.items():
        if len(file_list) > 1:
            # æŒ‰ä¼˜å…ˆçº§æ’åº
            sorted_files = sorted(file_list, key=lambda x: (
                get_image_priority_score(os.path.basename(x)),
                -file_sizes[x],  # æ–‡ä»¶å¤§å°å€’åº
                os.path.basename(x)  # æ–‡ä»¶åå­—æ¯åº
            ))

            # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåˆ é™¤å…¶ä»–
            keep_file = sorted_files[0]
            remove_files = sorted_files[1:]

            cleanup_plan['duplicate_removals'].append({
                'keep': keep_file,
                'remove': remove_files,
                'reason': f'é‡å¤æ–‡ä»¶ï¼Œä¿ç•™æœ€ä½³ç‰ˆæœ¬ (ä¼˜å…ˆçº§: {get_image_priority_score(os.path.basename(keep_file))})'
            })

    # å¤„ç†äº§å“å›¾ç‰‡å†²çª
    for product_id, image_list in conflicted_products.items():
        if len(image_list) > 3:  # å¦‚æœæŸä¸ªäº§å“å›¾ç‰‡è¿‡å¤š
            # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©æœ€ä½³çš„3å¼ 
            sorted_images = sorted(image_list, key=lambda x: (
                get_image_priority_score(os.path.basename(x)),
                -file_sizes[x],
                os.path.basename(x)
            ))

            keep_images = sorted_images[:3]
            remove_images = sorted_images[3:]

            if remove_images:
                cleanup_plan['conflict_resolutions'].append({
                    'product': product_id,
                    'keep': keep_images,
                    'remove': remove_images,
                    'reason': f'äº§å“å›¾ç‰‡è¿‡å¤šï¼Œä¿ç•™æœ€ä½³3å¼ '
                })

    # è¯†åˆ«å­¤ç«‹æ–‡ä»¶ï¼ˆæœªè¢«ä»»ä½•äº§å“ä½¿ç”¨çš„å›¾ç‰‡ï¼‰
    all_configured_images = set()
    for images in current_configs.values():
        all_configured_images.update(images)

    all_image_files = set(glob.glob(os.path.join(IMAGES_DIR, "*")))
    all_image_files = {f for f in all_image_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))}

    orphaned_files = all_image_files - all_configured_images

    # è¿‡æ»¤æ‰å¯èƒ½æœ‰ç”¨çš„æ–‡ä»¶
    filtered_orphaned = []
    for orphan in orphaned_files:
        filename = os.path.basename(orphan).lower()
        # ä¿ç•™å¯èƒ½æœ‰ç”¨çš„æ–‡ä»¶
        if not any(keyword in filename for keyword in ['official', 'new', 'placeholder']):
            if file_sizes[orphan] < 10000:  # å°äº10KBçš„æ–‡ä»¶å¯èƒ½æ˜¯æŸåçš„
                filtered_orphaned.append(orphan)

    cleanup_plan['orphaned_files'] = filtered_orphaned

    return cleanup_plan

def execute_cleanup_plan(cleanup_plan):
    """æ‰§è¡Œæ¸…ç†è®¡åˆ’"""
    print("ğŸ§¹ å¼€å§‹æ‰§è¡Œå›¾ç‰‡æ¸…ç†...")

    removed_count = 0
    backup_dir = os.path.join(SCRIPTS_DIR, 'image_backup')

    # åˆ›å»ºå¤‡ä»½ç›®å½•
    os.makedirs(backup_dir, exist_ok=True)

    # å¤„ç†é‡å¤æ–‡ä»¶åˆ é™¤
    for item in cleanup_plan['duplicate_removals']:
        print(f"\nğŸ“„ å¤„ç†é‡å¤æ–‡ä»¶ç»„:")
        print(f"   ä¿ç•™: {os.path.basename(item['keep'])}")

        for remove_file in item['remove']:
            try:
                filename = os.path.basename(remove_file)
                # å¤‡ä»½åˆ°å¤‡ä»½ç›®å½•
                backup_path = os.path.join(backup_dir, f"duplicate_{filename}")
                shutil.copy2(remove_file, backup_path)

                # åˆ é™¤åŸæ–‡ä»¶
                os.remove(remove_file)
                print(f"   âœ… åˆ é™¤: {filename} (å·²å¤‡ä»½)")
                removed_count += 1

            except Exception as e:
                print(f"   âŒ åˆ é™¤å¤±è´¥ {filename}: {e}")

    # å¤„ç†äº§å“å†²çªè§£å†³
    for item in cleanup_plan['conflict_resolutions']:
        print(f"\nğŸ·ï¸  å¤„ç†äº§å“ {item['product']} çš„å›¾ç‰‡å†²çª:")
        print(f"   ä¿ç•™ {len(item['keep'])} å¼ å›¾ç‰‡")

        for remove_file in item['remove']:
            try:
                filename = os.path.basename(remove_file)
                # å¤‡ä»½åˆ°å¤‡ä»½ç›®å½•
                backup_path = os.path.join(backup_dir, f"conflict_{filename}")
                shutil.copy2(remove_file, backup_path)

                # åˆ é™¤åŸæ–‡ä»¶
                os.remove(remove_file)
                print(f"   âœ… åˆ é™¤: {filename} (å·²å¤‡ä»½)")
                removed_count += 1

            except Exception as e:
                print(f"   âŒ åˆ é™¤å¤±è´¥ {filename}: {e}")

    # å¤„ç†å­¤ç«‹æ–‡ä»¶
    if cleanup_plan['orphaned_files']:
        print(f"\nğŸ—‘ï¸  æ¸…ç†å­¤ç«‹æ–‡ä»¶:")
        for orphan_file in cleanup_plan['orphaned_files']:
            try:
                filename = os.path.basename(orphan_file)
                # å¤‡ä»½åˆ°å¤‡ä»½ç›®å½•
                backup_path = os.path.join(backup_dir, f"orphaned_{filename}")
                shutil.copy2(orphan_file, backup_path)

                # åˆ é™¤åŸæ–‡ä»¶
                os.remove(orphan_file)
                print(f"   âœ… åˆ é™¤: {filename} (å·²å¤‡ä»½)")
                removed_count += 1

            except Exception as e:
                print(f"   âŒ åˆ é™¤å¤±è´¥ {filename}: {e}")

    return removed_count

def update_product_configurations():
    """æ›´æ–°äº§å“é…ç½®ä»¥åæ˜ å›¾ç‰‡æ¸…ç†ç»“æœ"""
    print("ğŸ”„ æ›´æ–°äº§å“é…ç½®...")

    updated_count = 0

    for html_file in glob.glob(os.path.join(PRODUCTS_DIR, "*.html")):
        product_id = Path(html_file).stem

        try:
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # è·å–è¯¥äº§å“å½“å‰ä»å­˜åœ¨çš„å›¾ç‰‡
            pattern = os.path.join(IMAGES_DIR, f"{product_id}*")
            available_images = []

            for img_path in glob.glob(pattern):
                if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):
                    filename = os.path.basename(img_path)
                    relative_path = f"../images/products/{filename}"
                    available_images.append(relative_path)

            # æŒ‰ä¼˜å…ˆçº§æ’åº
            available_images = sorted(available_images, key=lambda x:
                get_image_priority_score(os.path.basename(x)))

            if available_images:
                # æ›´æ–°data-imagesé…ç½®
                new_data_images = ",".join(available_images)

                # æŸ¥æ‰¾å¹¶æ›¿æ¢data-images
                data_images_pattern = r'data-images="[^"]*"'
                new_content = re.sub(data_images_pattern, f'data-images="{new_data_images}"', content)

                # æ›´æ–°ä¸»å›¾ç‰‡src
                main_image_pattern = r'(<img[^>]+class="main-image"[^>]+src=")[^"]*"'
                new_content = re.sub(main_image_pattern, f'\\1{available_images[0]}"', new_content)

                if new_content != content:
                    with open(html_file, 'w', encoding='utf-8') as f:
                        f.write(new_content)
                    print(f"   âœ… æ›´æ–° {product_id}: {len(available_images)} å¼ å›¾ç‰‡")
                    updated_count += 1

        except Exception as e:
            print(f"   âŒ æ›´æ–° {product_id} å¤±è´¥: {e}")

    return updated_count

def run_cleanup():
    """è¿è¡Œå®Œæ•´çš„å›¾ç‰‡æ¸…ç†æµç¨‹"""
    print("=" * 80)
    print("ğŸ§¹ å¼€å§‹æ¸…ç†é‡å¤å’Œé”™è¯¯å›¾ç‰‡")
    print("=" * 80)

    # 1. åˆ†æé‡å¤å›¾ç‰‡
    duplicate_groups, file_sizes = analyze_duplicate_images()

    # 2. è¯†åˆ«äº§å“å›¾ç‰‡å†²çª
    conflicted_products = identify_product_image_conflicts()

    # 3. åŠ è½½å½“å‰é…ç½®
    current_configs = load_current_configurations()

    # 4. åˆ›å»ºæ¸…ç†è®¡åˆ’
    cleanup_plan = create_cleanup_plan(duplicate_groups, conflicted_products, current_configs, file_sizes)

    # 5. æ˜¾ç¤ºæ¸…ç†è®¡åˆ’
    print(f"\nğŸ“‹ æ¸…ç†è®¡åˆ’:")
    print(f"   ğŸ”„ é‡å¤æ–‡ä»¶ç»„: {len(cleanup_plan['duplicate_removals'])}")
    print(f"   âš ï¸  äº§å“å†²çª: {len(cleanup_plan['conflict_resolutions'])}")
    print(f"   ğŸ—‘ï¸  å­¤ç«‹æ–‡ä»¶: {len(cleanup_plan['orphaned_files'])}")

    total_removals = 0
    for item in cleanup_plan['duplicate_removals']:
        total_removals += len(item['remove'])
    for item in cleanup_plan['conflict_resolutions']:
        total_removals += len(item['remove'])
    total_removals += len(cleanup_plan['orphaned_files'])

    print(f"\né¢„è®¡åˆ é™¤æ–‡ä»¶: {total_removals} ä¸ª")

    # 6. æ‰§è¡Œæ¸…ç†
    removed_count = execute_cleanup_plan(cleanup_plan)

    # 7. æ›´æ–°é…ç½®
    updated_count = update_product_configurations()

    print(f"\nğŸ¯ æ¸…ç†å®Œæˆ:")
    print(f"   åˆ é™¤æ–‡ä»¶: {removed_count} ä¸ª")
    print(f"   æ›´æ–°é…ç½®: {updated_count} ä¸ªäº§å“")
    print(f"   å¤‡ä»½ä½ç½®: scripts/image_backup/")

    print(f"\nğŸ“ å»ºè®®:")
    print(f"   1. æ£€æŸ¥æ¸…ç†ç»“æœ")
    print(f"   2. è¿è¡ŒéªŒè¯è„šæœ¬")
    print(f"   3. æµ‹è¯•äº§å“é¡µé¢")

if __name__ == "__main__":
    run_cleanup()